#!/usr/bin/env python3
"""
FastAPI ê¸°ë°˜ Sequence Classification ì¶”ë¡  API ì„œë²„
"""

import os
# ---- ë¡œê·¸/í”„ë¡œê·¸ë ˆìŠ¤ë°”/ë¹„ì „ ì˜ì¡´ ë¹„í™œì„±í™” (import ì „ì— ì„¤ì •) ----
os.environ.setdefault("TRANSFORMERS_NO_TORCHVISION", "1")
os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
os.environ.setdefault("PYTHONIOENCODING", "utf-8")

import logging
import warnings
from typing import List, Optional, Dict, Any
from contextlib import asynccontextmanager

# FastAPI ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# ë”¥ëŸ¬ë‹ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.getLogger().setLevel(logging.ERROR)
for name in ["transformers", "huggingface_hub", "urllib3", "filelock", "tqdm"]:
    logging.getLogger(name).setLevel(logging.ERROR)

# Transformers ë¡œê¹… ë„ê¸°
try:
    from transformers.utils import logging as hf_logging
    hf_logging.set_verbosity_error()
    try:
        hf_logging.disable_progress_bar()
    except Exception:
        pass
except Exception:
    pass

warnings.filterwarnings("ignore")

# ì „ì—­ ë³€ìˆ˜
classifier_pipeline = None
tokenizer = None
model = None
device = "cpu"

# Pydantic ëª¨ë¸ ì •ì˜
class TextInput(BaseModel):
    text: str = Field(..., description="ë¶„ë¥˜í•  í…ìŠ¤íŠ¸", min_length=1, max_length=1000)
    model_name: Optional[str] = Field(
        default="distilbert-base-uncased-finetuned-sst-2-english",
        description="ì‚¬ìš©í•  ëª¨ë¸ëª…"
    )
    method: str = Field(
        default="pipeline",
        description="ì¶”ë¡  ë°©ë²• (pipeline/manual)"
    )

class ClassificationResult(BaseModel):
    label: str = Field(..., description="ì˜ˆì¸¡ëœ ë¼ë²¨")
    score: float = Field(..., description="ì‹ ë¢°ë„ ì ìˆ˜ (0-1)")
    class_id: Optional[int] = Field(None, description="í´ë˜ìŠ¤ ID")
    model_name: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ëª…")
    method: str = Field(..., description="ì‚¬ìš©ëœ ì¶”ë¡  ë°©ë²•")

class BatchTextInput(BaseModel):
    texts: List[str] = Field(..., description="ë¶„ë¥˜í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸", min_items=1, max_items=100)
    model_name: Optional[str] = Field(
        default="distilbert-base-uncased-finetuned-sst-2-english",
        description="ì‚¬ìš©í•  ëª¨ë¸ëª…"
    )
    method: str = Field(
        default="pipeline",
        description="ì¶”ë¡  ë°©ë²• (pipeline/manual)"
    )

class BatchClassificationResult(BaseModel):
    results: List[ClassificationResult] = Field(..., description="ë¶„ë¥˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸")
    total_count: int = Field(..., description="ì´ í…ìŠ¤íŠ¸ ìˆ˜")
    success_count: int = Field(..., description="ì„±ê³µí•œ ë¶„ë¥˜ ìˆ˜")

class HealthResponse(BaseModel):
    status: str = Field(..., description="ì„œë²„ ìƒíƒœ")
    torch_available: bool = Field(..., description="PyTorch ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    transformers_available: bool = Field(..., description="Transformers ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    device: str = Field(..., description="í˜„ì¬ ì‚¬ìš© ë””ë°”ì´ìŠ¤")
    model_loaded: bool = Field(..., description="ëª¨ë¸ ë¡œë“œ ìƒíƒœ")

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def normalize_pipeline_device(dev: str) -> int:
    """pipeline device: CPU=-1, GPU=0 (auto ì²˜ë¦¬ í¬í•¨)"""
    if dev == "auto":
        return 0 if (TORCH_AVAILABLE and torch.cuda.is_available()) else -1
    if isinstance(dev, str):
        d = dev.lower()
        if d == "cpu":
            return -1
        if d.startswith("cuda"):
            if ":" in d:
                try:
                    return int(d.split(":", 1)[1])
                except ValueError:
                    return 0
            return 0
        return -1
    if isinstance(dev, int):
        return dev
    return -1

def to_torch_device(dev: str) -> str:
    if not TORCH_AVAILABLE:
        return "cpu"
    if dev == "auto":
        return "cuda" if torch.cuda.is_available() else "cpu"
    return dev

def load_model_and_tokenizer(model_name: str, device: str):
    """ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        dev = to_torch_device(device)
        if TORCH_AVAILABLE:
            model = model.to(dev)
        return tokenizer, model, dev
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")

def predict_with_pipeline(text: str, model_name: str, device: str = "auto") -> Dict[str, Any]:
    """Pipeline ì¶”ë¡ """
    try:
        dev_idx = normalize_pipeline_device(device)
        clf = pipeline(
            "sentiment-analysis",
            model=model_name,
            device=dev_idx
        )
        result = clf(text)
        return result[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Pipeline ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")

def predict_manual(text: str, tokenizer, model, device: str) -> Dict[str, Any]:
    """ìˆ˜ë™ ì¶”ë¡  (PyTorch í•„ìš”)"""
    try:
        if not TORCH_AVAILABLE:
            raise HTTPException(status_code=500, detail="manual ëª¨ë“œëŠ” PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            idx = logits.argmax(dim=-1).item()
            conf = probs[0][idx].item()
        
        label = model.config.id2label.get(idx, f"Class_{idx}")
        return {"label": label, "score": conf, "class_id": idx}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìˆ˜ë™ ì¶”ë¡  ì‹¤íŒ¨: {str(e)}")

# ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ì‹œ
    global device
    device = to_torch_device("auto")
    print(f"ğŸš€ API ì„œë²„ ì‹œì‘ - ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ”¥ PyTorch ì‚¬ìš© ê°€ëŠ¥: {TORCH_AVAILABLE}")
    print(f"ğŸ¤– Transformers ì‚¬ìš© ê°€ëŠ¥: {TRANSFORMERS_AVAILABLE}")
    
    yield
    
    # ì¢…ë£Œì‹œ
    print("ğŸ›‘ API ì„œë²„ ì¢…ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Sequence Classification API",
    description="Hugging Face Transformersë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.get("/", response_model=Dict[str, str])
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "Sequence Classification API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return HealthResponse(
        status="healthy",
        torch_available=TORCH_AVAILABLE,
        transformers_available=TRANSFORMERS_AVAILABLE,
        device=device,
        model_loaded=classifier_pipeline is not None or model is not None
    )

@app.post("/classify", response_model=ClassificationResult)
async def classify_text(input_data: TextInput):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    if not TRANSFORMERS_AVAILABLE:
        raise HTTPException(status_code=500, detail="Transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    try:
        if input_data.method == "pipeline":
            result = predict_with_pipeline(input_data.text, input_data.model_name, device)
            return ClassificationResult(
                label=result["label"],
                score=result["score"],
                class_id=None,
                model_name=input_data.model_name,
                method=input_data.method
            )
        else:  # manual
            global tokenizer, model
            if tokenizer is None or model is None:
                tokenizer, model, dev = load_model_and_tokenizer(input_data.model_name, device)
            
            result = predict_manual(input_data.text, tokenizer, model, dev)
            return ClassificationResult(
                label=result["label"],
                score=result["score"],
                class_id=result.get("class_id"),
                model_name=input_data.model_name,
                method=input_data.method
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/classify/batch", response_model=BatchClassificationResult)
async def classify_texts_batch(input_data: BatchTextInput):
    """ë°°ì¹˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    if not TRANSFORMERS_AVAILABLE:
        raise HTTPException(status_code=500, detail="Transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    results = []
    success_count = 0
    
    try:
        for text in input_data.texts:
            try:
                if input_data.method == "pipeline":
                    result = predict_with_pipeline(text, input_data.model_name, device)
                    results.append(ClassificationResult(
                        label=result["label"],
                        score=result["score"],
                        class_id=None,
                        model_name=input_data.model_name,
                        method=input_data.method
                    ))
                else:  # manual
                    global tokenizer, model
                    if tokenizer is None or model is None:
                        tokenizer, model, dev = load_model_and_tokenizer(input_data.model_name, device)
                    
                    result = predict_manual(text, tokenizer, model, dev)
                    results.append(ClassificationResult(
                        label=result["label"],
                        score=result["score"],
                        class_id=result.get("class_id"),
                        model_name=input_data.model_name,
                        method=input_data.method
                    ))
                success_count += 1
            except Exception as e:
                # ê°œë³„ í…ìŠ¤íŠ¸ ì‹¤íŒ¨ì‹œì—ë„ ê³„ì† ì§„í–‰
                results.append(ClassificationResult(
                    label="ERROR",
                    score=0.0,
                    class_id=None,
                    model_name=input_data.model_name,
                    method=input_data.method
                ))
        
        return BatchClassificationResult(
            results=results,
            total_count=len(input_data.texts),
            success_count=success_count
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/info")
async def get_models_info():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´"""
    return {
        "default_model": "distilbert-base-uncased-finetuned-sst-2-english",
        "supported_models": [
            "distilbert-base-uncased-finetuned-sst-2-english",
            "cardiffnlp/twitter-roberta-base-sentiment-latest",
            "nlptown/bert-base-multilingual-uncased-sentiment"
        ],
        "torch_available": TORCH_AVAILABLE,
        "device": device
    }

if __name__ == "__main__":
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
