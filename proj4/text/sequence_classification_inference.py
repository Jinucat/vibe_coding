#!/usr/bin/env python3
"""
Sequence Classification ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (ê¹”ë” ì¶œë ¥ ëª¨ë“œ)
- ê¸°ë³¸: ì“¸ëª¨ì—†ëŠ” ë¡œê·¸/í”„ë¡œê·¸ë ˆìŠ¤ë°” ìˆ¨ê¹€
- --verbose ì¼ ë•Œë§Œ ìµœì†Œí•œì˜ ë””ë²„ê·¸ ì¶œë ¥
"""

import os
# ---- ë¡œê·¸/í”„ë¡œê·¸ë ˆìŠ¤ë°”/ë¹„ì „ ì˜ì¡´ ë¹„í™œì„±í™” (import ì „ì— ì„¤ì •) ----
os.environ.setdefault("TRANSFORMERS_NO_TORCHVISION", "1")      # torchvision ê°•ì œ ë¹„í™œì„±í™”
os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")     # ë‹¤ìš´ë¡œë“œ ë°” ìˆ¨ê¹€
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")         # í—ˆê¹…í˜ì´ìŠ¤ í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±
os.environ.setdefault("PYTHONIOENCODING", "utf-8")             # ì¶œë ¥ ì¸ì½”ë”© ê³ ì •

import argparse
import warnings
import sys, traceback, logging

# ---- íŒŒì´ì¬ ë¡œê±° ë ˆë²¨ ì „ì²´ ë‚®ì¶¤(ê¸°ë³¸ ERROR) ----
logging.getLogger().setLevel(logging.ERROR)
for name in ["transformers", "huggingface_hub", "urllib3", "filelock", "tqdm"]:
    logging.getLogger(name).setLevel(logging.ERROR)

# ---- ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ----
try:
    import torch
    TORCH_AVAILABLE = True
except Exception:
    TORCH_AVAILABLE = False

# Transformers ë¡œê¹…/í”„ë¡œê·¸ë ˆìŠ¤ë°” ë„ê¸°
try:
    from transformers.utils import logging as hf_logging
    hf_logging.set_verbosity_error()
    try:
        hf_logging.disable_progress_bar()
    except Exception:
        pass
except Exception:
    pass

try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    TRANSFORMERS_AVAILABLE = True
except Exception as e:
    print("âŒ Transformers ì„í¬íŠ¸ ì‹¤íŒ¨:", repr(e))
    traceback.print_exc()
    sys.exit(2)

warnings.filterwarnings("ignore")  # íŒŒì´ì¬ ê²½ê³  ìˆ¨ê¹€


def normalize_pipeline_device(dev):
    """pipeline device: CPU=-1, GPU=0 (auto ì²˜ë¦¬ í¬í•¨)"""
    if dev == "auto":
        return 0 if (TORCH_AVAILABLE and torch.cuda.is_available()) else -1
    if isinstance(dev, str):
        d = dev.lower()
        if d == "cpu":
            return -1
        if d.startswith("cuda"):
            if ":" in d:
                try:
                    return int(d.split(":", 1)[1])
                except ValueError:
                    return 0
            return 0
        return -1
    if isinstance(dev, int):
        return dev
    return -1


def to_torch_device(dev):
    if not TORCH_AVAILABLE:
        return "cpu"
    if dev == "auto":
        return "cuda" if torch.cuda.is_available() else "cpu"
    return dev


def load_model_and_tokenizer(model_name, device):
    """ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ (ì¡°ìš©íˆ)"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        dev = to_torch_device(device)
        if TORCH_AVAILABLE:
            model = model.to(dev)
        return tokenizer, model, dev
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None, None, None


def predict_with_pipeline(text, model_name, device):
    """Pipeline ì¶”ë¡  (ì¡°ìš©íˆ)"""
    try:
        dev_idx = normalize_pipeline_device(device)
        clf = pipeline(
            "sentiment-analysis",
            model=model_name,
            device=dev_idx
        )
        return clf(text)
    except Exception as e:
        print(f"âŒ Pipeline ì¶”ë¡  ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def predict_manual(text, tokenizer, model, device):
    """ìˆ˜ë™ ì¶”ë¡  (PyTorch í•„ìš”)"""
    try:
        if not TORCH_AVAILABLE:
            raise RuntimeError("manual ëª¨ë“œëŠ” PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            idx = logits.argmax(dim=-1).item()
            conf = probs[0][idx].item()
        label = model.config.id2label.get(idx, f"Class_{idx}")
        return {"label": label, "score": conf, "class_id": idx}
    except Exception as e:
        print(f"âŒ ìˆ˜ë™ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(
        description="Sequence Classification ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (ê¹”ë” ì¶œë ¥)"
    )
    parser.add_argument("--text", type=str, required=True, help="ë¶„ë¥˜í•  í…ìŠ¤íŠ¸")
    parser.add_argument(
        "--model", type=str,
        default="distilbert-base-uncased-finetuned-sst-2-english",
        help="ëª¨ë¸ëª…(ê¸°ë³¸: distilbert-base-uncased-finetuned-sst-2-english)"
    )
    parser.add_argument(
        "--method", type=str, choices=["pipeline", "manual"], default="pipeline",
        help="ì¶”ë¡  ë°©ë²• (pipeline/manual)"
    )
    parser.add_argument(
        "--device", type=str, default="auto", choices=["auto", "cpu", "cuda"],
        help="ë””ë°”ì´ìŠ¤ (auto/cpu/cuda)"
    )
    parser.add_argument("--verbose", action="store_true", help="ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥")
    args = parser.parse_args()

    # verboseê°€ ì•„ë‹ˆë©´ ìš°ë¦¬ê°€ ì¶œë ¥í•˜ëŠ” ìµœì†Œ ë©”ì‹œì§€ë§Œ
    if args.verbose:
        print("ğŸš€ Sequence Classification ì¶”ë¡  ì‹œì‘")
        print(f"ğŸ“ í…ìŠ¤íŠ¸: {args.text}")
        print(f"ğŸ¤– ëª¨ë¸: {args.model}")
        print(f"âš™ï¸  ë°©ë²•: {args.method}")
        print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {args.device}")
        print("-" * 50)
        dev = to_torch_device(args.device)
        print(f"ğŸ”§ ì‹¤ì œ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {dev}")
        if TORCH_AVAILABLE:
            print(f"ğŸ”¥ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")

    if args.method == "pipeline":
        result = predict_with_pipeline(args.text, args.model, args.device)
        if result:
            item = result[0]
            print(f"{item['label']} {item['score']:.4f}")
        else:
            print("âŒ ì¶”ë¡  ì‹¤íŒ¨")
    else:
        tokenizer, model, dev = load_model_and_tokenizer(args.model, args.device)
        if tokenizer and model:
            result = predict_manual(args.text, tokenizer, model, dev)
            if result:
                print(f"{result['label']} {result['score']:.4f}")
            else:
                print("âŒ ì¶”ë¡  ì‹¤íŒ¨")
        else:
            print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
